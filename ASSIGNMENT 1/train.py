# -*- coding: utf-8 -*-
"""w5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MO_jU1g8fWE9K6PgNXihZpSLFNGneAH0

## Importing Libraries And Downloading Data
"""

import numpy as np
import pandas as pd
from keras.datasets import fashion_mnist
import matplotlib.pyplot as plt
import random
import math
from sklearn.model_selection import train_test_split
import subprocess
subprocess.call(['pip', 'install', 'wandb'])
import wandb

#!pip install --upgrade wandb
#pip install --upgrade wandb
#!pip install wandb -qqq

wandb.login()
wandb.init(project = "Deep Learning Assignment")

"""### Splitting data"""

#splitting the data

(x1,y1),(x2,y2) = fashion_mnist.load_data()
x1 = x1 / 255.0
x2 = x2 / 255.0
x_train1,x_test2,y_train1,y_test2 = train_test_split(x1 , y1 , test_size = 0.1 , random_state = 0)

"""### Data preprocessing"""

x_train = x_train1.reshape(54000,784).T
y1_train = y_train1.reshape(54000,1).T
x_test  = x_test2.reshape(6000,784).T
y1_test  = y_test2.reshape(6000,1).T
print("without one-hot encoding")
print("shape of x_train :",x_train.shape)
print("shape of y_train :",y1_train.shape)
print("shape of x_test  :",x_test.shape)
print("shape of y_test  :",y1_test.shape)

#one hot encoding

y_train = np.zeros((10,y1_train.shape[1]))
for i in range(0,y1_train.shape[1]):
    for j in range(0,10):
        if y1_train[0,i] == j:
            y_train[j,i] = 1
            
y_test = np.zeros((10,y1_test.shape[1]))
for i in range(0,y1_test.shape[1]):
    for j in range(0,10):
        if y1_test[0,i] == j:
            y_test[j,i] = 1

num_classes = y_train.shape[0]
class_name = {0: "T-shirt/top", 1: "Trouser", 2: "Pullover", 3: "Dress", 4: "Coat", 5: "Sandal", 6: "Shirt", 7: "Sneaker", 8: "Bag", 9: "Ankle boot"}
print("after one_hot encoding")
print("number of classes:",num_classes)
print("shape of x_train :",x_train.shape)
print("shape of y_train :",y_train.shape)
print("shape of x_test  :",x_test.shape)
print("shape of y_test  :",y_test.shape)

"""### Logging sample images"""

# images_list = []
# classes_list = []
# i = 0

# for j in range(y1_train.shape[1]):
        
#         if y1_train[0,j] == i and i<=9:
#             images = x_train[:,j].reshape(28,28)
#             images_list.append(images)
#             classes_list.append(class_name[y1_train[0,j]])
#             i = i+1
#         else:
#             j = j+1
            
# for i in range(num_classes):
    
#     plt.imshow(images_list[i])
#     plt.title(classes_list[i])
#     plt.show()

# wandb.log({"Qn 1":[wandb.Image(img,caption = caption) for img,caption in zip(images_list,classes_list)]})

"""## Neural Network models

#### Activation functions and its derivative
"""

# Defining Activation function for neural network

def sigmoid_function(x):
    exp = np.exp(-x)
    return 1/(1+exp)

def sigmoid_derivative(x):
    return sigmoid_function(x) * (1-sigmoid_function(x))

def tanh_function(x):
    return np.tanh(x)

def tanh_derivative(x):
    return (1 - (np.tanh(x)**2))

def ReLu(x):
    return np.maximum(0,x)

def ReLu_derivative(x):
    return 1*(x>0) 

def softmax_function(x):
    
    exps = np.exp(x - np.max(x , axis=0, keepdims = True))
    return exps / np.sum(exps, axis=0 , keepdims = True)

def softmax_derivative(x):
    return softmax_function(x) * (1-(softmax_function(x)))

def cost_function(al,y,Batch_size,loss,lamb,parameters):
    al = np.clip(al, 1e-9, 1 - 1e-9)                                                    # Clip to avoid taking the log of 0 or 1
    if loss == 'cross_entropy':
        if y.shape[0] == 1:                                                             # binary classification 
            cost = (1/Batch_size) * (-np.dot(y,np.log(al).T) - np.dot(1-y, np.log(1-al).T))
        else:                                                                           # multiclass-classification
            cost = -(1/Batch_size) * np.sum(y * np.log(al))
    elif loss == 'mse':
         cost = (1/2) * np.sum((y-al)**2)/Batch_size
    acc = 0
    for i in range(1, len(parameters)//2 + 1):
        acc += np.sum(parameters["W"+str(i)]**2)
    cost = cost + (lamb/(2*Batch_size))*acc
    cost = np.squeeze(cost)      
    return cost

def accuracy(inp, labels, parameters):
    forward_prop = Forward_Propagation(inp, parameters , activation_function)
    a_out = forward_prop['a2']   # containes propabilities with shape(10, 1)
    a_out = np.argmax(a_out, 0)  # 0 represents row wise 
    labels = np.argmax(labels, 0)
    acc = np.mean(a_out == labels)*100
    
    return acc

"""### INITIALIZING PARAMETERS"""

def init_parameters(layers , init_mode):  #initializing parameters
    ''' Function to initialize weights, biases and previous updates of Neural_Network
    
    Parameters
    ----------
    layers : List
        list of numbers of neurons per layer specifying layer dimensions in the format [#inp_features,#num_neurons in layer1,#num_neurons in layer2,......,#out_layer]
    
    init_mode : String
        initialization mode ('Random_normal','Random_uniform','Xavier')
    
    Returns
    -------
    Parameters : Dictionary
         contains weights and biases
    
    Previous_Updates : Dictionary   
         used for different purposes for different optimizers
    
    ''' 
    np.random.seed(42)
    Parameters = {}
    Previous_Updates = {}
    L = len(layers)           #no.of layers
    
    for l in range(1, L):     #except the last activation layer
        if init_mode == 'Random_normal':
            Parameters['W'+str(l)] = np.random.randn(layers[l],layers[l-1])
            
        elif init_mode == 'Random_uniform':
            Parameters['W'+str(l)] = np.random.rand(layers[l],layers[l-1])  
            
        elif init_mode == 'Xavier':
            Parameters['W'+str(l)] = np.random.randn(layers[l],layers[l-1])*np.sqrt(2/(layers[l]+layers[l-1]))
            
            
        Parameters['b'+str(l)] = np.zeros((layers[l],1))
        
        Previous_Updates['W'+str(l)] = np.zeros((layers[l],layers[l-1]))
        Previous_Updates['b'+str(l)] = np.zeros((layers[l], 1))
        
    return Parameters,Previous_Updates

"""### FORWARD PROPAGATION"""

def Forward_Propagation(x, Parameters, activation_function):
    '''Function to forward propagate a minibatch of data once through the NN

    Parameters
    -----------
    x: numpy array
        data in (features,batch_size) format

    Parameters: Dictionary
        Weights(W) and biases(b) of the Neural Network

    activation_function: String
        activation function to be used except the output layer where it takes accordingly(Sigmoid,softmax) based on the type of classification

    Returns
    --------
    output: numpy array
        contains the output probabilities for each class and each data sample after only one pass
    h: numpy array
        contains all post-activations
    a: numpy array
        contains all pre-activations

    '''
   
    forward_prop = {}
    L = math.floor(len(Parameters)/2)                  
    
    #first activation layer will be input layer itself
    
    forward_prop['h0'] = x 
    
    # tanh or ReLu activation functions are used for l-1 layers
    for l in range(1, L):
        
    # ai+1 = Wi+1 * hij + bi+1   
        forward_prop['a' + str(l)] = np.dot(Parameters['W' + str(l)],forward_prop['h' + str(l-1)]) + Parameters['b' + str(l)]
        
        if activation_function == 'tanh':
            forward_prop['h' + str(l)] = tanh_function(forward_prop['a' + str(l)])
        elif activation_function == 'ReLu':
            forward_prop['h' + str(l)] = ReLu(forward_prop['a' + str(l)])
        elif activation_function == 'sigmoid':
            forward_prop['h' + str(l)] = sigmoid_function(forward_prop['a' + str(l)])

    forward_prop['a' + str(L)] = np.matmul(Parameters['W' + str(L)],forward_prop['h' + str(L-1)]) + Parameters['b' + str(L)]
    
    # sigmoid or softmax functions are used for output layer
    if forward_prop['a' + str(L)].shape[0] == 1:  #if it is a binary output then sigmoid function
        forward_prop['h' + str(L)] = sigmoid_function(forward_prop['a' + str(L)])
    else :
        forward_prop['h' + str(L)] = softmax_function(forward_prop['a' + str(L)]) #if it is multiclass classification then it is softmax function
    
    return forward_prop['h' + str(L)], forward_prop

"""### BACK PROPAGATION"""

def back_prop(al,y,Batch_size,loss,Parameters,forward_prop,activation_function,lamb):
    '''Function to calculate gradients for a minibatch of data once through the Neural Network through back_propagation

    Parameters
    ----------
    al: numpy array
        output from forward propagation

    y: numpy array
        actual class labels
     
    h: numpy array
        post-activations

    a: numpy array
        pre-activations   

    Parameters: Dictionary
        contains W and b on the NeuralNetwork   

    activation_function: String
        activation function to be used except the output layer where it takes accordingly(Sigmoid,softmax) based on the type of classification

    Batch_size: int
        mini-batch-size

    loss: String
        loss function ('mse','crossentropy')

    lamb: Float
        L2 regularisation lambda

    Returns
    -------
    gradients: Dictionary
        gradients wrt weights(W) and biases(b)

    '''
    
    gradients = {}
    L = math.floor(len(Parameters)/2)

    if loss == 'cross_entropy':
        gradients["da" + str(L)] = al - y
    elif loss == 'mse':
        gradients["da" + str(L)] = (al - y) * softmax_derivative(forward_prop['a' + str(L)])    
    gradients["dW" + str(L)] = (1/Batch_size) * (np.dot(gradients["da" + str(L)],forward_prop['h' + str(L-1)].T) + lamb*Parameters["W"+str(L)])
    gradients["db" + str(L)] = (1/Batch_size) * (np.sum(gradients["da" + str(L)], axis = 1, keepdims = True))
    
    for l in reversed(range(1, L)):
        if activation_function == 'tanh':
            gradients["da" + str(l)] = np.dot(Parameters['W' + str(l+1)].T,gradients["da" + str(l+1)])*tanh_derivative(forward_prop['h' + str(l)]) 
        elif activation_function == 'ReLu':
            gradients["da" + str(l)] = np.dot(Parameters['W' + str(l+1)].T,gradients["da" + str(l+1)])*ReLu_derivative(forward_prop['h' + str(l)])
        elif activation_function == 'sigmoid':
            gradients["da" + str(l)] = np.dot(Parameters['W' + str(l+1)].T,gradients["da" + str(l+1)])*sigmoid_derivative(forward_prop['h' + str(l)])
            
        gradients["dW" + str(l)] = 1/(Batch_size) *(np.dot(gradients["da" + str(l)],forward_prop['h' + str(l-1)].T) + lamb*Parameters["W"+str(l)])
        gradients["db" + str(l)] = 1/(Batch_size) *(np.sum(gradients["da" + str(l)], axis = 1, keepdims = True)) 

    return gradients

"""### UPDATING PARAMETERS FOR VARIOUS OPTIMIZERS"""

def Update_Parameters_sgd(Parameters, gradients, learning_rate):
    ''' Update W and b of the Neural Network according to sgd updates

    Parameters
    ----------
    Parameters: Dictionary
        contains weights(W) and biases(b) of the NeuralNetwork

    gradients: Dictionary
        contains gradients wrt weights(W) and biases(b) returned by backpropagation

    learning_rate: Float
        learning rate

    Returns
    -------
    Parameters: Dictionary
        updated NeuralNetwork parameters

    '''

    L = math.floor(len(Parameters)/2) 
    
    for l in range(L):
        Parameters["W" + str(l+1)] = Parameters["W" + str(l+1)] - learning_rate * gradients["dW" + str(l+1)]
        Parameters["b" + str(l+1)] = Parameters["b" + str(l+1)] - learning_rate * gradients["db" + str(l+1)]
        
    return Parameters

def Update_Parameters_mgd(Parameters,gradients,learning_rate,Previous_Updates,beta):
    ''' Update W and b of the Neural Network according to momentum updates

    Parameters
    ----------
    Parameters: Dictionary
        contains weights(W) and biases(b) of the Neural Network

    gradients: Dictionary
        contains gradients wrt weights(W) and biases(b) returned by backpropagation

    learning_rate: Float
        learning rate
    
    beta: Float
        decay rate

    Previous_Updates: Dictionary
        contains previous weights(W) and biases(b) values, accumulated in a weighted fashion along with the gradients eg.
        Previous_Updates[Wi] = beta*Previous_Updates[Wi] + (1-beta)*gradients[dWi]

    Returns
    -------
    Parameters: Dictionary
        updated NeuralNetwork Parameters

    Previous_Updates: Dictionary
        updated previous updates 

    '''
    
    L = math.floor(len(Parameters)/2)
    
    for l in range(L):
        Previous_Updates["W"+str(l+1)] = beta*Previous_Updates["W"+str(l+1)] + learning_rate*gradients["dW" + str(l+1)]
        Previous_Updates["b"+str(l+1)] = beta*Previous_Updates["b"+str(l+1)] + learning_rate*gradients["db" + str(l+1)]
        Parameters["W"+str(l+1)] = Parameters["W"+str(l+1)] - Previous_Updates["W"+str(l+1)]
        Parameters["b"+str(l+1)] = Parameters["b"+str(l+1)] - Previous_Updates["b"+str(l+1)]
        
    return Parameters , Previous_Updates

def Update_Parameters_nagd(Parameters,gradients,learning_rate,Previous_Updates,beta):
    ''' Update W and b of the Neural Network according to nestrov acccelerated gradient descent updates

    Parameters
    ----------
    Parameters: Dictionary
        contains weights(W) and biases(b) of the Neural Network

    gradients: Dictionary
        contains gradients wrt weights(W) and biases(b) returned by backpropagation

    learning_rate: Float
        learning rate
    
    beta: Float
        decay rate

    Previous_Updates: Dictionary
        contains previous weights(W) and biases(b) values, accumulated in a weighted fashion along with the gradients eg.
        Previous_Updates[Wi] = beta*Previous_Updates[Wi] + (1-beta)*gradients[dWi]

    Returns
    -------
    Parameters: Dictionary
        updated NeuralNetwork Parameters

    Previous_Updates: Dictionary
        updated previous updates 

    '''
    
    L = math.floor(len(Parameters)/2)
    
    for l in range(L):
        Previous_Updates["W"+str(l+1)] = beta*Previous_Updates["W"+str(l+1)] + learning_rate*gradients["dW" + str(l+1)]
        Previous_Updates["b"+str(l+1)] = beta*Previous_Updates["b"+str(l+1)] + learning_rate*gradients["db" + str(l+1)]
        Parameters["W"+str(l+1)] = Parameters["W"+str(l+1)] - Previous_Updates["W"+str(l+1)]
        Parameters["b"+str(l+1)] = Parameters["b"+str(l+1)] - Previous_Updates["b"+str(l+1)]
        
        
    return Parameters , Previous_Updates

def Update_Parameters_RMSprop(Parameters,gradients,learning_rate,Previous_Updates,beta,v):
    ''' Update W and b of the Neural Network according to RMSprop updates

    Parameters
    ----------
    Parameters: Dictionary
        contains weights(W) and biases(b) of the Neural Network

    gradients: Dictionary
        contains gradients wrt weights(W) and biases(b) returned by backpropagation

    learning_rate: Float
        learning rate
    
    beta: Float
        decay rate

    v: Dictionary
        contains previous weights(W) and biases(b) values, accumulated in a weighted fashion along with the gradients square eg.
        v[Wi] = beta*v[Wi] + (1-beta)*(gradients[dWi]^2)

    Returns
    -------
    Parameters: Dictionary
        updated Neural Network parameters

    v: Dictionary
        updated "velocities"

    '''
    L = math.floor(len(Parameters)/2)
    delta = 0.000001
    for l in range(L):
        vdw = beta*v["W" + str(l+1)] + (1-beta)*np.multiply(gradients["dW" + str(l+1)],gradients["dW" + str(l+1)])
        vdb = beta*v["b" + str(l+1)] + (1-beta)*np.multiply(gradients["db" + str(l+1)],gradients["db" + str(l+1)])

        Parameters["W" + str(l+1)] = Parameters["W" + str(l+1)] - learning_rate * gradients["dW" + str(l+1)] / (np.sqrt(vdw)+delta)
        Parameters["b" + str(l+1)] = Parameters["b" + str(l+1)] - learning_rate * gradients["db" + str(l+1)] / (np.sqrt(vdb)+delta)

        v["W" + str(l+1)] = vdw
        v["b" + str(l+1)] = vdb

    return Parameters,v

def Update_Parameters_adam(Parameters,gradients,learning_rate,v,m,t):
    ''' Update W and b of the Neural Network according to adam updates

    Parameters
    ----------
    Parameters: Dictionary
        contains weights(W) and biases(b) of the Neural Network

    gradients: Dictionary
        contains gradients wrt weights(W) and biases(b) returned by backpropagation

    learning_rate: Float
        learning rate

    v: Dictionary
        contains previous weights(W) and biases(b) values, accumulated in a weighted fashion along with the gradients eg.
        v[Wi] = beta1*v[Wi] + (1-beta1)*(gradient[dWi])

    m: Dictionary
        contains previous weights(W) and biases(b) values, accumulated in a weighted fashion along with the gradients^2 eg.
        v[Wi] = beta2*v[Wi] + (1-beta2)*(gradient[dWi]^2)

    t: int
        timestep for Adam

    Returns
    -------
    Parameters: Dictionary
        updated Neural Network parameters

    v: Dictionary
        updated previous updates

    m: Dictionary
        updated "velocities"

    t: int
        updated timestep

    '''
    L = math.floor(len(Parameters)/2)
    epsilon = 0.00000001
    beta1 = 0.9
    beta2 = 0.999
    for l in range(L):
        mdw = beta1*m["W" + str(l+1)] + (1-beta1)*gradients["dW" + str(l+1)]
        vdw = beta2*v["W" + str(l+1)] + (1-beta2)*np.square(gradients["dW" + str(l+1)])
        
        vw_hat = vdw/(1-beta2**t)
        mw_hat = mdw/(1-beta1**t)
        

        Parameters["W" + str(l+1)] = Parameters["W" + str(l+1)] - (learning_rate * mw_hat)/np.sqrt(vw_hat+epsilon)
        
        mdb = beta1*m["b"+str(l+1)] + (1-beta1)*gradients["db"+str(l+1)]
        vdb = beta2*v["b"+str(l+1)] + (1-beta2)*np.square(gradients["db"+str(l+1)])
        mb_hat = mdb/(1-beta1**t)
        vb_hat = vdb/(1-beta2**t)
        
        Parameters["b" + str(l+1)] = Parameters["b" + str(l+1)] - (learning_rate * mb_hat)/np.sqrt(vb_hat+epsilon)
        
        v["dW" + str(l+1)] = vdw
        m["dW" + str(l+1)] = mdw
        v["db" + str(l+1)] = vdb
        m["db" + str(l+1)] = mdb

    return Parameters,v,m,t

def Update_Parameters_nadam(Parameters,gradients,learning_rate,v,m,t):
    
    ''' Update W and b of the Neural Network according to nadam updates

    Parameters
    ----------
    Parameters: Dictionary
        contains weights(W) and biases(b) of the Neural Network

    gradients: Dictionary
        contains gradients wrt weights(W) and biases(b) returned by backpropagation

    learning_rate: Float
        learning rate

    v: Dictionary
        contains previous weights(W) and biases(b) values, accumulated in a weighted fashion along with the gradients eg.
        v[Wi] = beta1*v[Wi] + (1-beta1)*(gradient[dWi])

    m: Dictionary
        contains previous weights(W) and biases(b) values, accumulated in a weighted fashion along with the gradients^2 eg.
        v[Wi] = beta2*v[Wi] + (1-beta2)*(gradient[dWi]^2)

    t: int
        timestep for nadam

    Returns
    -------
    Parameters: Dictionary
        updated Neural Network parameters

    v: Dictionary
        updated previous updates

    m: Dictionary
        updated "velocities"

    t: int
        updated timestep

    '''
    
    L = math.floor(len(Parameters)/2)
    epsilon = 0.00000001
    beta1 = 0.9
    beta2 = 0.999
    for l in range(L):
        mdw = beta1*m["W" + str(l+1)] + (1-beta1)*gradients["dW" + str(l+1)]
        vdw = beta2*v["W" + str(l+1)] + (1-beta2)*np.square(gradients["dW" + str(l+1)])
        
        vw_hat = vdw/(1-beta2**t)
        mw_hat = mdw/(1-beta1**t)
        

        Parameters["W" + str(l+1)] = Parameters["W" + str(l+1)] - (learning_rate * mw_hat)/np.sqrt(vw_hat+epsilon)
        
        mdb = beta1*m["b"+str(l+1)] + (1-beta1)*gradients["db"+str(l+1)]
        vdb = beta2*v["b"+str(l+1)] + (1-beta2)*np.square(gradients["db"+str(l+1)])
        mb_hat = mdb/(1-beta1**t)
        vb_hat = vdb/(1-beta2**t)
        
        Parameters["b" + str(l+1)] = Parameters["b" + str(l+1)] - (learning_rate * mb_hat)/np.sqrt(vb_hat+epsilon)
        
        v["dW" + str(l+1)] = vdw
        m["dW" + str(l+1)] = mdw
        v["db" + str(l+1)] = vdb
        m["db" + str(l+1)] = mdb

    return Parameters,v,m,t

def predict(x, y, Parameters, activation_function):

    m = x.shape[1]
    y_pred, caches = Forward_Propagation(x, Parameters, activation_function)
    
    if y.shape[0] == 1:
        y_pred = np.array(y_pred > 0.5, dtype = 'float')
    else:
        y = np.argmax(y, 0)
        y_pred = np.argmax(y_pred, 0)
        accuracy = np.round(np.sum(y == y_pred)/m, 2)
        
       
    return accuracy

def plot_graph_acc(train,validation,iterations):

    plt.plot(iterations,train,c='red',label='Training Accuracy')
    plt.plot(iterations,validation,c='lime',label='Validation Accuracy')
    plt.title("Training and validation Accuracy vs number of epochs",size=14)
    plt.xlabel("Number of epochs",size =14)
    plt.ylabel("Accuracy",size=14)
    plt.grid()
    plt.legend()
    plt.show()

def plot_graph_loss(train,validation,iterations):

    plt.plot(iterations,train,c='red',label='Training Loss')
    plt.plot(iterations,validation,c='lime',label='Validation Loss')
    plt.title("Training and validation Loss vs number of epochs",size=14)
    plt.xlabel("Number of epochs",size =14)
    plt.ylabel("Loss",size=14)
    plt.grid()
    plt.legend()
    plt.show()

"""### STOCHASTIC GRADIENT DESCENT"""

def stochastic_gradient_descent(x_train,y_train,x_val,y_val,Batch_size,layers,learning_rate,lamb,loss,activation_function,iterations):
    np.random.seed(1)
    cost_train_sgd = []
    cost_val_sgd = []
    iteration_sgd = []
    train_sgd = []
    validation_sgd = []
    
    Parameters, Previous_Updates = init_parameters(layers, init_mode='Random_normal')

    for i in range(iterations):
        iteration_sgd.append(i)
         
        for j in range(0, x_train.shape[1], Batch_size):
            Batch_count = Batch_size
            
            if j+Batch_size > x_train.shape[1]:
                Batch_count = x_train.shape[1] - j 
            
            al, forward_prop = Forward_Propagation(x_train[:, j:j+Batch_count].reshape(x_train.shape[0], Batch_count), Parameters, activation_function)
            gradients = back_prop(al, y_train[:, j:j+Batch_count].reshape(y_train.shape[0], Batch_count), Batch_count,loss,Parameters, forward_prop, activation_function, lamb)
            Parameters = Update_Parameters_sgd(Parameters, gradients, learning_rate)
        
        al_train, forward_prop_train = Forward_Propagation(x_train, Parameters, activation_function)
        cost_train = cost_function(al_train, y_train, y_train.shape[1], loss, lamb, Parameters)
        cost_train_sgd.append(cost_train)
        accuracy_train_sgd = predict(x_train, y_train, Parameters, activation_function)
        train_sgd.append(accuracy_train_sgd)
        
        al_val, forward_prop_val = Forward_Propagation(x_val, Parameters, activation_function)
        cost_val = cost_function(al_val, y_val, y_val.shape[1], loss, lamb, Parameters)
        cost_val_sgd.append(cost_val)
        accuracy_val_sgd = predict(x_val, y_val, Parameters, activation_function)
        validation_sgd.append(accuracy_val_sgd)
        
        wandb.log({"training_accuracy":accuracy_train_sgd,"Validation_accuracy":accuracy_val_sgd,"training_loss":cost_train,"Validation_loss":cost_val})
        
        if i % (iterations/10) == 0:
            
            print("\niter:{} \t cost_train: {:.2f} \t cost_val: {:.2f} \t train_acc: {:.2f} \t val_acc: {:.2f}".format(i, cost_train,cost_val,accuracy_train_sgd,accuracy_val_sgd))

        
    plot_graph_acc(train_sgd, validation_sgd, iteration_sgd)
    plot_graph_loss(cost_train_sgd, cost_val_sgd, iteration_sgd)
            
    return Parameters

"""### MOMENTUM GRADIENT DESCENT """

def momentum_gradient_descent(x_train,y_train,x_val,y_val,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations):

    np.random.seed(1)
    cost_train_mgd = []
    cost_val_mgd = []
    iteration_mgd = []
    train_mgd = []
    validation_mgd = []
    Parameters, Previous_Updates = init_parameters(layers , init_mode = 'Random_normal')
    
    for i in range(0, iterations):
        iteration_mgd.append(i)
        
        for j in range(0,x_train.shape[1],Batch_size):
            
            Batch_count = Batch_size
                
            if j+Batch_size > x_train.shape[1]:
                      Batch_count = x_train.shape[1] - j 
            
            al, forward_prop = Forward_Propagation(x_train[:,j:j+Batch_count].reshape(x_train.shape[0],Batch_count), Parameters, activation_function)
            gradients = back_prop(al, y_train[:,j:j+Batch_count].reshape(y_train.shape[0],Batch_count),Batch_count,loss,Parameters, forward_prop, activation_function,lamb)
            Parameters,Previous_Updates = Update_Parameters_mgd(Parameters, gradients, learning_rate,Previous_Updates,beta)
        
        al_train, forward_prop_train = Forward_Propagation(x_train, Parameters, activation_function)
        cost_train = cost_function(al_train,y_train,y_train.shape[1],loss,lamb,Parameters)
        cost_train_mgd.append(cost_train)
        
        al_val, forward_prop_val = Forward_Propagation(x_val, Parameters, activation_function)
        cost_val = cost_function(al_val, y_val, y_val.shape[1], loss, lamb, Parameters)
        cost_val_mgd.append(cost_val)
        
        accuracy_train_mgd = predict(x_train, y_train, Parameters, activation_function)
        accuracy_val_mgd = predict(x_test, y_test, Parameters, activation_function)
        train_mgd.append(accuracy_train_mgd)
        validation_mgd.append(accuracy_val_mgd)
        
        wandb.log({"training_accuracy":accuracy_train_mgd,"Validation_accuracy":accuracy_val_mgd,"training_loss":cost_train,"Validation_loss":cost_val})
        
        if i % (iterations/10) == 0:
            
            print("\niter:{} \t cost_train: {:.2f} \t cost_val: {:.2f} \t train_acc: {:.2f} \t val_acc: {:.2f}".format(i, cost_train,cost_val,accuracy_train_mgd,accuracy_val_mgd))

    plot_graph_acc(train_mgd,validation_mgd,iteration_mgd)
    plot_graph_loss(cost_train_mgd,cost_val_mgd,iteration_mgd)
        
    return Parameters

"""### NESTEROV ACCELERATED GRADIENT DESCENT"""

def nesterov_accelerated_gradient_descent(x_train,y_train,x_val,y_val,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations):

    np.random.seed(1)
    cost_train_nagd = []
    cost_val_nagd = []
    iteration_nagd = []
    train_nagd = []
    validation_nagd = []
    Parameters, Previous_Updates = init_parameters(layers , init_mode = 'Random_normal')
    Parameters_look_ahead = Parameters.copy() 
    L = math.floor(len(Parameters)/2)
    
    for i in range(0, iterations):
        iteration_nagd.append(i)
        
        for j in range(0,x_train.shape[1],Batch_size):
            Batch_count = Batch_size
            if j+Batch_size > x_train.shape[1]:
                    Batch_count = x_train.shape[1] - j 
            
            for l in range(L):
                Parameters_look_ahead["W"+str(l+1)] = Parameters["W"+str(l+1)] - beta*Previous_Updates["W"+str(l+1)]
                Parameters_look_ahead["b"+str(l+1)] = Parameters["b"+str(l+1)] - beta*Previous_Updates["b"+str(l+1)]
            
            al,forward_prop = Forward_Propagation(x_train[:,j:j+Batch_count].reshape(x_train.shape[0],Batch_count), Parameters, activation_function)
            gradients = back_prop(al, y_train[:,j:j+Batch_count].reshape(y_train.shape[0],Batch_count),Batch_count,loss,Parameters_look_ahead, forward_prop, activation_function,lamb)
            Parameters,Previous_Updates = Update_Parameters_nagd(Parameters_look_ahead, gradients, learning_rate,Previous_Updates,beta)
        
        al_train, forward_prop_train = Forward_Propagation(x_train, Parameters, activation_function)
        cost_train = cost_function(al_train,y_train,y_train.shape[1],loss,lamb,Parameters)
        cost_train_nagd.append(cost_train)
        al_val, forward_prop_val = Forward_Propagation(x_val, Parameters, activation_function)
        cost_val = cost_function(al_val, y_val, y_val.shape[1], loss, lamb, Parameters)
        cost_val_nagd.append(cost_val)
        
        accuracy_train_nagd = predict(x_train, y_train, Parameters, activation_function)
        accuracy_val_nagd = predict(x_test, y_test, Parameters, activation_function)
        train_nagd.append(accuracy_train_nagd)
        validation_nagd.append(accuracy_val_nagd)
        
        wandb.log({"training_accuracy":accuracy_train_nagd,"Validation_accuracy":accuracy_val_nagd,"training_loss":cost_train,"Validation_loss":cost_val})
        
        if i % (iterations/10) == 0:

            print("\niter:{} \t cost_train: {:.2f} \t cost_val: {:.2f} \t train_acc: {:.2f} \t val_acc: {:.2f}".format(i, cost_train,cost_val,accuracy_train_nagd,accuracy_val_nagd))

        
    plot_graph_acc(train_nagd,validation_nagd,iteration_nagd)
    plot_graph_loss(cost_train_nagd,cost_val_nagd,iteration_nagd)
        
    return Parameters

"""### RMS"""

def RMS_prop(x_train,y_train,x_val,y_val,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations):
    cost_train_RMS = []
    cost_val_RMS = []
    iteration_RMS = []
    train_RMS = []
    validation_RMS = []

    
    Parameters, Previous_Updates = init_parameters(layers , init_mode = 'Random_normal')
    Parameters_look_ahead = Parameters.copy()
    v = Previous_Updates.copy()
    L = math.floor(len(Parameters)/2)
    
    for i in range(0, iterations):
        iteration_RMS.append(i)
        for j in range(0,x_train.shape[1],Batch_size):
            Batch_count = Batch_size
            if j+Batch_size > x_train.shape[1]:
                Batch_count = x_train.shape[1] - j 
            al, forward_prop = Forward_Propagation(x_train[:,j:j+Batch_count].reshape(x_train.shape[0],Batch_count), Parameters, activation_function)
            gradients = back_prop(al, y_train[:,j:j+Batch_count].reshape(y_train.shape[0],Batch_count),Batch_count,loss,Parameters, forward_prop, activation_function,lamb)
            Parameters,Previous_Updates = Update_Parameters_RMSprop(Parameters, gradients, learning_rate,Previous_Updates,beta,v)
        
        al_train, forward_prop_train = Forward_Propagation(x_train, Parameters, activation_function)
        cost_train = cost_function(al_train,y_train,y_train.shape[1],loss,lamb,Parameters)
        cost_train_RMS.append(cost_train)
        al_val, forward_prop_val = Forward_Propagation(x_val, Parameters, activation_function)
        cost_val = cost_function(al_val, y_val, y_val.shape[1], loss, lamb, Parameters)
        cost_val_RMS.append(cost_val)
        
        accuracy_train_RMS = predict(x_train, y_train, Parameters, activation_function)
        accuracy_val_RMS = predict(x_test, y_test, Parameters, activation_function)
        train_RMS.append(accuracy_train_RMS)
        validation_RMS.append(accuracy_val_RMS)
        
        wandb.log({"training_accuracy":accuracy_train_RMS,"Validation_accuracy":accuracy_val_RMS,"training_loss":cost_train,"Validation_loss":cost_val})
        
        if i % (iterations/10) == 0:
            
            print("\niter:{} \t cost_train: {:.2f} \t cost_val: {:.2f} \t train_acc: {:.2f} \t val_acc: {:.2f}".format(i, cost_train,cost_val,accuracy_train_RMS,accuracy_val_RMS))

        
    plot_graph_acc(train_RMS,validation_RMS,iteration_RMS)
    plot_graph_loss(cost_train_RMS,cost_val_RMS,iteration_RMS)
        
    return Parameters

"""### ADAM """

def adam(x_train,y_train,x_val,y_val,Batch_size,layers,learning_rate,lamb,loss,activation_function,iterations):
    cost_train_ad = []
    cost_val_ad = []
    iteration_ad = []
    train_ad = []
    validation_ad = []
    Parameters, Previous_Updates = init_parameters(layers , init_mode = 'Random_normal')
    Parameters_look_ahead = Parameters.copy()
    t = 1
    v = Previous_Updates.copy()
    m = Previous_Updates.copy()
    L = math.floor(len(Parameters)/2)
    
    for i in range(0, iterations):
        iteration_ad.append(i)
        for j in range(0,x_train.shape[1],Batch_size):
            Batch_count = Batch_size
            if j+Batch_size > x_train.shape[1]:
                Batch_count = x_train.shape[1] - j 
            al, forward_prop = Forward_Propagation(x_train[:,j:j+Batch_count].reshape(x_train.shape[0],Batch_count),Parameters,activation_function)
            gradients = back_prop(al, y_train[:,j:j+Batch_count].reshape(y_train.shape[0],Batch_count),Batch_count,loss,Parameters, forward_prop, activation_function,lamb)
            Parameters,v,m,t = Update_Parameters_adam(Parameters,gradients,learning_rate,v,m,t)
        
        al_train, forward_prop_train = Forward_Propagation(x_train,Parameters,activation_function)
        cost_train = cost_function(al_train,y_train,y_train.shape[1],loss,lamb,Parameters)
        cost_train_ad.append(cost_train)
        al_val, forward_prop_val = Forward_Propagation(x_val, Parameters, activation_function)
        cost_val = cost_function(al_val,y_val,y_val.shape[1],loss,lamb,Parameters)
        cost_val_ad.append(cost_val)
        accuracy_train_ad = predict(x_train, y_train, Parameters, activation_function)
        accuracy_val_ad = predict(x_test, y_test, Parameters, activation_function)
        train_ad.append(accuracy_train_ad)
        validation_ad.append(accuracy_val_ad)
        
        wandb.log({"training_accuracy":accuracy_train_ad,"Validation_accuracy":accuracy_val_ad,"training_loss":cost_train,"Validation_loss":cost_val})
        
        if i%(iterations/10) == 0:
                print("\niter:{} \t cost_train: {:.2f} \t cost_val: {:.2f} \t train_acc: {:.2f} \t val_acc: {:.2f}".format(i, cost_train,cost_val,accuracy_train_ad,accuracy_val_ad))

    plot_graph_acc(train_ad,validation_ad,iteration_ad)
    plot_graph_loss(cost_train_ad,cost_val_ad,iteration_ad)
        
    return Parameters

"""### NADAM"""

def nadam(x_train,y_train,x_val,y_val,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations):

    np.random.seed(1)
    cost_train_nadam = []
    cost_val_nadam = []
    iteration_nadam = []
    train_nadam = []
    validation_nadam = []
    Parameters, Previous_Updates = init_parameters(layers , init_mode = 'Random_normal')
    Parameters_look_ahead = Parameters.copy()
    t = 1
    v = Previous_Updates.copy()
    m = Previous_Updates.copy()
    L = math.floor(len(Parameters)/2)
    
    for i in range(0, iterations):
        iteration_nadam.append(i)
        
        for l in range(L):
                Parameters_look_ahead["W"+str(l+1)] = Parameters["W"+str(l+1)]-beta*Previous_Updates["W"+str(l+1)]
                Parameters_look_ahead["b"+str(l+1)] = Parameters["b"+str(l+1)]-beta*Previous_Updates["b"+str(l+1)]
        
        for j in range(0,x_train.shape[1],Batch_size):
            Batch_count = Batch_size
            
            for l in range(L):
                Parameters_look_ahead["W"+str(l+1)] = Parameters["W"+str(l+1)]-beta*Previous_Updates["W"+str(l+1)]
                Parameters_look_ahead["b"+str(l+1)] = Parameters["b"+str(l+1)]-beta*Previous_Updates["b"+str(l+1)]
            
            if j+Batch_size > x_train.shape[1]:
                Batch_count = x_train.shape[1] - j 
            
            al, forward_prop = Forward_Propagation(x_train[:,j:j+Batch_count].reshape(x_train.shape[0],Batch_count), Parameters, activation_function)
            gradients = back_prop(al, y_train[:,j:j+Batch_count].reshape(y_train.shape[0],Batch_count),Batch_count,loss,Parameters, forward_prop, activation_function,lamb)
            Parameters,v,m,t = Update_Parameters_nadam(Parameters,gradients,learning_rate,v,m,t)
        
        al_train, forward_prop_train = Forward_Propagation(x_train, Parameters, activation_function)
        cost_train = cost_function(al_train,y_train,y_train.shape[1],loss,lamb,Parameters)
        cost_train_nadam.append(cost_train)
        al_val, forward_prop_val = Forward_Propagation(x_val, Parameters, activation_function)
        cost_val = cost_function(al_val, y_val, y_val.shape[1], loss, lamb, Parameters)
        cost_val_nadam.append(cost_val)
        accuracy_train_nadam = predict(x_train, y_train, Parameters, activation_function)
        accuracy_val_nadam = predict(x_test, y_test, Parameters, activation_function)
        train_nadam.append(accuracy_train_nadam)
        validation_nadam.append(accuracy_val_nadam)
        
        wandb.log({"training_accuracy":accuracy_train_nadam,"Validation_accuracy":accuracy_val_nadam,"training_loss":cost_train,"Validation_loss":cost_val})
        
        
        if i%(iterations/10) == 0:
            print("\niter:{} \t cost_train: {:.2f} \t cost_val: {:.2f} \t train_acc: {:.2f} \t val_acc: {:.2f}".format(i, cost_train,cost_val,accuracy_train_nadam,accuracy_val_nadam))

                
    plot_graph_acc(train_nadam,validation_nadam,iteration_nadam)
    plot_graph_loss(cost_train_nadam,cost_val_nadam,iteration_nadam)
        
    return Parameters

"""#### Hyperparameter tuning using sweeps"""

def Neural_Network():
    
    config_defaults = {
        'iterations':5,
        'Batch_size':64,
        'learning_rate':0.001,
        'activation_function':'ReLu',
        'loss':'cross_entropy',
        'optimizer':'RMS',
        'init_mode':'Xavier',
        'lamb': 0,
        'num_neurons':64,
        'num_hidden': 3 
        }
    
    wandb.init(config=config_defaults)
    config = wandb.config
    
    num_neurons = config.num_neurons
    init_mode = config.init_mode
    num_hidden = config.num_hidden
    iterations = config.iterations
    Batch_size = config.Batch_size
    learning_rate = config.learning_rate
    activation_function = config.activation_function
    loss = config.loss
    lamb = config.lamb
    optimizer = config.optimizer
    
    run_name = "learningrate:{} \t activationfunction:{} \t init_mode:{} \t batchsize:{} \t optimizer:{} \t L2:{} \t epochs:{} \t num_hidden:{} \t num_neurons:{}".format(learning_rate,activation_function,init_mode,Batch_size,optimizer,lamb,iterations,num_hidden,num_neurons) 
    print(run_name)
    layers = [x_train.shape[0]] + [num_neurons]*num_hidden + [y_train.shape[0]]
    beta =0.9
    
    
    if optimizer == 'stochastic':
        Parameters = stochastic_gradient_descent(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,lamb,loss,activation_function,iterations)

    elif optimizer == 'momentum':
        Parameters = momentum_gradient_descent(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations)
    
    elif optimizer == 'nesterov':
        Parameters = nesterov_accelerated_gradient_descent(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations)

    elif optimizer == 'RMS':
        Parameters = RMS_prop(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations)
  
    elif optimizer == 'adam':
        Parameters = adam(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,lamb,loss,activation_function,iterations)

    elif optimizer == 'nadam':
        Parameters = nadam(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations)

        
        
    wandb.run.name =run_name 
    wandb.run.save()
    wandb.run.finish()
# Neural_Network()

import argparse
def train(args):
  print(0)
  wandb.init(project =args.wandb_project,entity = args.wandb_entity)
  # wandb.init(config=config_defaults)
  # config = wandb.config
  
  num_neurons = args.num_neurons
  init_mode = args.init_mode
  num_hidden = args.num_hidden
  iterations = args.iterations
  Batch_size = args.Batch_size
  learning_rate = args.learning_rate
  activation_function = args.activation_function
  loss = args.loss
  lamb = args.lamb
  optimizer = args.optimizer
  
  run_name = "learningrate:{} \t activationfunction:{} \t init_mode:{} \t batchsize:{} \t optimizer:{} \t L2:{} \t epochs:{} \t num_hidden:{} \t num_neurons:{}".format(learning_rate,activation_function,init_mode,Batch_size,optimizer,lamb,iterations,num_hidden,num_neurons) 
  print(run_name)
  layers = [x_train.shape[0]] + [num_neurons]*num_hidden + [y_train.shape[0]]
  beta =0.9
  
  print(1)
  if optimizer == 'stochastic':
      Parameters = stochastic_gradient_descent(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,lamb,loss,activation_function,iterations)

  elif optimizer == 'momentum':
      Parameters = momentum_gradient_descent(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations)
  
  elif optimizer == 'nesterov':
      Parameters = nesterov_accelerated_gradient_descent(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations)

  elif optimizer == 'RMS':
      Parameters = RMS_prop(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations)

  elif optimizer == 'adam':
      Parameters = adam(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,lamb,loss,activation_function,iterations)

  elif optimizer == 'nadam':
      Parameters = nadam(x_train,y_train,x_test,y_test,Batch_size,layers,learning_rate,beta,lamb,loss,activation_function,iterations)

      
      
  wandb.run.name =run_name 
  wandb.run.save()

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument("-wp", "--wandb_project", default="dharani")
    parser.add_argument("-we", "--wandb_entity", default="dharanivendhanv01")###
    parser.add_argument("-e", "--iterations", default=20, type=int)
    parser.add_argument("-b", "--Batch_size", default=64, type=int)
    parser.add_argument("-l", "--loss", default="cross_entropy", choices=["cross_entropy", "mse"])
    parser.add_argument("-o", "--optimizer", default="RMS", choices=["stochastic", "momentum", "nesterov", "RMS", "adam", "nadam"])
    parser.add_argument("-lr", "--learning_rate", default=0.001, type=float)
    parser.add_argument("-w_d", "--lamb", default=0.05, type=float)
    parser.add_argument("-w_i", "--init_mode", default="Random_normal", choices=["Random_normal","Random_uniform",  "Xavier"])
    parser.add_argument("-nhl", "--num_hidden", default=1, type=int)
    parser.add_argument("-sz", "--num_neurons", default=32, type=int)
    parser.add_argument("-a", "--activation_function", default="ReLu", choices=["ReLu", "sigmoid", "tanh"])

    args = parser.parse_args()

    train(args)
    wandb.finish()