{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a917e5ff",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f25d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19caa15e",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304c839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x1,y1),(x2,y2) = fashion_mnist.load_data()\n",
    "x1 = x1 / 255.0\n",
    "x2 = x2 / 255.0\n",
    "x_train1,x_test2,y_train1,y_test2 = train_test_split(x1 , y1 , test_size = 0.1 , random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353660f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without one-hot encoding\n",
      "shape of x_train : (784, 54000)\n",
      "shape of y_train : (1, 54000)\n",
      "shape of x_test  : (784, 6000)\n",
      "shape of y_test  : (1, 6000)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train1.reshape(54000,784).T\n",
    "y1_train = y_train1.reshape(54000,1).T\n",
    "x_test  = x_test2.reshape(6000,784).T\n",
    "y1_test  = y_test2.reshape(6000,1).T\n",
    "print(\"without one-hot encoding\")\n",
    "print(\"shape of x_train :\",x_train.shape)\n",
    "print(\"shape of y_train :\",y1_train.shape)\n",
    "print(\"shape of x_test  :\",x_test.shape)\n",
    "print(\"shape of y_test  :\",y1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f090c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "\n",
    "y_train = np.zeros((10,y1_train.shape[1]))\n",
    "for i in range(0,y1_train.shape[1]):\n",
    "    for j in range(0,10):\n",
    "        if y1_train[0,i] == j:\n",
    "            y_train[j,i] = 1\n",
    "            \n",
    "y_test = np.zeros((10,y1_test.shape[1]))\n",
    "for i in range(0,y1_test.shape[1]):\n",
    "    for j in range(0,10):\n",
    "        if y1_test[0,i] == j:\n",
    "            y_test[j,i] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8fcb28",
   "metadata": {},
   "source": [
    "### DEFINIG ACTIVATION FUNCTIONS AND LOSS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ee3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Activation function for neural network\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    exp = np.exp(-x)\n",
    "    return 1/(1+exp)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid_function(x) * (1-sigmoid_function(x))\n",
    "\n",
    "def tanh_function(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (np.tanh(x)**2))\n",
    "\n",
    "def ReLu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def ReLu_derivative(x):\n",
    "    return 1*(x>0) \n",
    "\n",
    "def softmax_function(x):\n",
    "    \n",
    "    exps = np.exp(x - np.max(x , axis=0, keepdims = True))\n",
    "    return exps / np.sum(exps, axis=0 , keepdims = True)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    return softmax_function(x) * (1-(softmax_function(x)))\n",
    "\n",
    "def cost_function(al,y,Batch_size,loss,lamb,parameters):\n",
    "    al = np.clip(al, 1e-9, 1 - 1e-9)                                                    # Clip to avoid taking the log of 0 or 1\n",
    "    if loss == 'cross_entropy':\n",
    "        if y.shape[0] == 1:                                                             # binary classification \n",
    "            cost = (1/Batch_size) * (-np.dot(y,np.log(al).T) - np.dot(1-y, np.log(1-al).T))\n",
    "        else:                                                                           # multiclass-classification\n",
    "            cost = -(1/Batch_size) * np.sum(y * np.log(al))\n",
    "    elif loss == 'mse':\n",
    "         cost = (1/2) * np.sum((y-al)**2)/Batch_size\n",
    "    acc = 0\n",
    "    for i in range(1, len(parameters)//2 + 1):\n",
    "        acc += np.sum(parameters[\"W\"+str(i)]**2)\n",
    "    cost = cost + (lamb/(2*Batch_size))*acc\n",
    "    cost = np.squeeze(cost)      \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0163d20a",
   "metadata": {},
   "source": [
    "### INITIALIZING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857968e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layers , init_mode):  \n",
    "    ''' Function to initialize weights, biases and previous updates of Neural_Network\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    layers : List\n",
    "        list of numbers of neurons per layer specifying layer dimensions in the format [#inp_features,#num_neurons in layer1,#num_neurons in layer2,......,#out_layer]\n",
    "    \n",
    "    init_mode : String\n",
    "        initialization mode ('Random_normal','Random_uniform','Xavier')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Parameters : Dictionary\n",
    "         contains weights and biases\n",
    "    \n",
    "    Previous_Updates : Dictionary   \n",
    "         used for different purposes for different optimizers\n",
    "    \n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    Parameters = {}\n",
    "    Previous_Updates = {}\n",
    "    L = len(layers)           #no.of layers\n",
    "    \n",
    "    for l in range(1, L):     #except the last activation layer\n",
    "        if init_mode == 'Random_normal':\n",
    "            Parameters['W'+str(l)] = np.random.randn(layers[l],layers[l-1])\n",
    "            \n",
    "        elif init_mode == 'Random_uniform':\n",
    "            Parameters['W'+str(l)] = np.random.rand(layers[l],layers[l-1])  \n",
    "            \n",
    "        elif init_mode == 'Xavier':\n",
    "            Parameters['W'+str(l)] = np.random.randn(layers[l],layers[l-1])*np.sqrt(2/(layers[l]+layers[l-1]))\n",
    "            \n",
    "            \n",
    "        Parameters['b'+str(l)] = np.zeros((layers[l],1))\n",
    "        \n",
    "        Previous_Updates['W'+str(l)] = np.zeros((layers[l],layers[l-1]))\n",
    "        Previous_Updates['b'+str(l)] = np.zeros((layers[l], 1))\n",
    "        \n",
    "    return Parameters,Previous_Updates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7b032",
   "metadata": {},
   "source": [
    "### FORWARD PROPAGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ec7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward_Propagation(x, Parameters, activation_function):\n",
    "    '''Function to forward propagate a minibatch of data once through the NN\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    x: numpy array\n",
    "        data in (features,batch_size) format\n",
    "\n",
    "    Parameters: Dictionary\n",
    "        Weights(W) and biases(b) of the Neural Network\n",
    "\n",
    "    activation_function: String\n",
    "        activation function to be used except the output layer where it takes accordingly(Sigmoid,softmax) based on the type of classification\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    output: numpy array\n",
    "        contains the output probabilities for each class and each data sample after only one pass\n",
    "    h: numpy array\n",
    "        contains all post-activations\n",
    "    A: numpy array\n",
    "        contains all pre-activations\n",
    "\n",
    "    '''\n",
    "   \n",
    "    forward_prop = {}\n",
    "    L = math.floor(len(Parameters)/2)                  \n",
    "    \n",
    "    #first activation layer will be input layer itself\n",
    "    \n",
    "    forward_prop['h0'] = x \n",
    "    \n",
    "    # tanh or ReLu activation functions are used for l-1 layers\n",
    "    for l in range(1, L):\n",
    "        \n",
    "    # ai+1 = Wi+1 * hij + bi+1   \n",
    "        forward_prop['a' + str(l)] = np.dot(Parameters['W' + str(l)],forward_prop['h' + str(l-1)]) + Parameters['b' + str(l)]\n",
    "        \n",
    "        if activation_function == 'tanh':\n",
    "            forward_prop['h' + str(l)] = tanh_function(forward_prop['a' + str(l)])\n",
    "        elif activation_function == 'ReLu':\n",
    "            forward_prop['h' + str(l)] = ReLu(forward_prop['a' + str(l)])\n",
    "        elif activation_function == 'sigmoid':\n",
    "            forward_prop['h' + str(l)] = sigmoid_function(forward_prop['a' + str(l)])\n",
    "\n",
    "    forward_prop['a' + str(L)] = np.matmul(Parameters['W' + str(L)],forward_prop['h' + str(L-1)]) + Parameters['b' + str(L)]\n",
    "    \n",
    "    # sigmoid or softmax functions are used for output layer\n",
    "    if forward_prop['a' + str(L)].shape[0] == 1:  #if it is a binary output then sigmoid function\n",
    "        forward_prop['h' + str(L)] = sigmoid_function(forward_prop['a' + str(L)])\n",
    "    else :\n",
    "        forward_prop['h' + str(L)] = softmax_function(forward_prop['a' + str(L)]) #if it is multiclass classification then it is softmax function\n",
    "    \n",
    "    return forward_prop['h' + str(L)], forward_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520087b",
   "metadata": {},
   "source": [
    "### FEED_FORWARD FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d6d2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x,y,layers,init_mode,loss,activation_function):\n",
    "    \n",
    "    Parameters,Previous_Updates = init_parameters(layers , init_mode)\n",
    "    al, forward_prop = Forward_Propagation(x, Parameters, activation_function)\n",
    "    \n",
    "    return al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ede47712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons in hidden layers1 95\n",
      "neurons in hidden layers2 90\n",
      "predicted output : [0.03896708 0.08704719 0.30094989 0.03719906 0.10418865 0.13478975\n",
      " 0.21114578 0.01945331 0.02087356 0.04538574]\n"
     ]
    }
   ],
   "source": [
    "layers = [x_train.shape[0],y_train.shape[0]]\n",
    "num_neuron = [95,90]\n",
    "\n",
    "for i in range(len(num_neuron)):\n",
    "    layers.insert(i+1,num[i])\n",
    "for j in range(len(num_neuron)):\n",
    "    print('neurons in hidden layers'+str(j+1) ,num_neuron[j])\n",
    "\n",
    "y_pred = feed_forward(x_train,y_train,layers,init_mode='Xavier',loss='mse',activation_function='sigmoid')\n",
    "print('predicted output :',y_pred[:,0]) #for one data point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
